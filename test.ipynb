{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import onnx, onnxruntime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def get_dataloader(train, batch_size, num_workers=2):\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                    transforms.Normalize(0.5, 0.5)])\n",
    "    dataset = torchvision.datasets.MNIST(root='./data',\n",
    "                                         train=train,\n",
    "                                         download=True,\n",
    "                                         transform=transform)\n",
    "    return torch.utils.data.DataLoader(dataset,\n",
    "                                       batch_size=batch_size,\n",
    "                                       shuffle=train,\n",
    "                                       num_workers=num_workers)\n",
    "\n",
    "\n",
    "trainloader = get_dataloader(train=True, batch_size=batch_size, num_workers=8)\n",
    "testloader = get_dataloader(train=False, batch_size=batch_size, num_workers=8)\n",
    "\n",
    "images, labels = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(2304, 512)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "device = 'mps'\n",
    "model = CustomCNN()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.073\n",
      "[1,   200] loss: 0.027\n",
      "[1,   300] loss: 0.019\n",
      "[1,   400] loss: 0.016\n",
      "[2,   100] loss: 0.012\n",
      "[2,   200] loss: 0.011\n",
      "[2,   300] loss: 0.010\n",
      "[2,   400] loss: 0.009\n",
      "[3,   100] loss: 0.008\n",
      "[3,   200] loss: 0.007\n",
      "[3,   300] loss: 0.007\n",
      "[3,   400] loss: 0.006\n",
      "[4,   100] loss: 0.006\n",
      "[4,   200] loss: 0.005\n",
      "[4,   300] loss: 0.005\n",
      "[4,   400] loss: 0.005\n",
      "[5,   100] loss: 0.005\n",
      "[5,   200] loss: 0.004\n",
      "[5,   300] loss: 0.004\n",
      "[5,   400] loss: 0.004\n",
      "[6,   100] loss: 0.004\n",
      "[6,   200] loss: 0.004\n",
      "[6,   300] loss: 0.004\n",
      "[6,   400] loss: 0.004\n",
      "[7,   100] loss: 0.003\n",
      "[7,   200] loss: 0.004\n",
      "[7,   300] loss: 0.003\n",
      "[7,   400] loss: 0.003\n",
      "[8,   100] loss: 0.003\n",
      "[8,   200] loss: 0.003\n",
      "[8,   300] loss: 0.003\n",
      "[8,   400] loss: 0.003\n",
      "[9,   100] loss: 0.003\n",
      "[9,   200] loss: 0.003\n",
      "[9,   300] loss: 0.003\n",
      "[9,   400] loss: 0.003\n",
      "[10,   100] loss: 0.003\n",
      "[10,   200] loss: 0.003\n",
      "[10,   300] loss: 0.002\n",
      "[10,   400] loss: 0.003\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.detach().item()\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 98 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "assert correct / total >= 0.98\n",
    "print(f'Test accuracy: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './logs/test_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomCNN()\n",
    "model.load_state_dict(torch.load('./logs/test_model.pth', weights_only=True))\n",
    "model.to('cpu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model ONNx export\n",
    "\n",
    "To prepare the model for later quantization, some layers are fused before exportation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_layers(model):\n",
    "    grouped_layers = []\n",
    "    prev_layer = None\n",
    "    prev_name = None\n",
    "\n",
    "    for module_name, module in model.named_children():\n",
    "        layer_name = module.__class__.__name__\n",
    "\n",
    "        if prev_layer:\n",
    "            # Conv2d or Linear followed by an Activation\n",
    "            if prev_layer in {\"Conv2d\", \"Linear\"} and layer_name in {\"ReLU\", \"LeakyReLU\", \"Sigmoid\", \"Tanh\"}:\n",
    "                grouped_layers.append([prev_name, module_name])\n",
    "                prev_layer, prev_name = None, None  # Reset\n",
    "\n",
    "            # Conv2d followed by Normalization\n",
    "            elif prev_layer == \"Conv2d\" and layer_name in {\"BatchNorm2d\", \"LayerNorm\", \"InstanceNorm2d\"}:\n",
    "                prev_layer, prev_name = layer_name, module_name  # Store temporarily\n",
    "\n",
    "            # Normalization followed by Activation\n",
    "            elif prev_layer in {\"BatchNorm2d\", \"LayerNorm\", \"InstanceNorm2d\"} and layer_name in {\"ReLU\", \"LeakyReLU\", \"Sigmoid\", \"Tanh\"}:\n",
    "                grouped_layers.append([prev_name, module_name])\n",
    "                prev_layer, prev_name = None, None  # Reset\n",
    "\n",
    "            else:\n",
    "                prev_layer, prev_name = layer_name, module_name  # Move to next layer\n",
    "        else:\n",
    "            prev_layer, prev_name = layer_name, module_name  # Initialize\n",
    "\n",
    "    return grouped_layers\n",
    "\n",
    "grouped_layers = group_layers(model)\n",
    "fused_model = torch.quantization.fuse_modules(\n",
    "    model,\n",
    "    grouped_layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `CustomCNN([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `CustomCNN([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n"
     ]
    }
   ],
   "source": [
    "tensor_x = torch.randn((1, 1, 28, 28))\n",
    "onnx_program = torch.onnx.export(\n",
    "                    fused_model, \n",
    "                    (tensor_x,), \n",
    "                    dynamo=True,\n",
    "                    export_params=True,        \n",
    "                    opset_version=18,          \n",
    "                    do_constant_folding=True,  \n",
    "                    input_names = ['input'],                          \n",
    "                    output_names = ['output'],\n",
    "                )\n",
    "onnx_program.save(\"./exports/mnist.onnx\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Model quantization\n",
    "\n",
    "For better quantization, we fuse some of the layers first.\n",
    "ONNx model should also be pre-processed before quantization with : \n",
    "\n",
    "```bash\n",
    "python -m onnxruntime.quantization.preprocess --input ./exports/mnist.onnx --output ./exports/mnist-infer.onnx\n",
    "```\n",
    "\n",
    "Quantization requires tensor shape information to perform its best. Model optimization also improve the performance of quantization. For instance, a Convolution node followed by a BatchNormalization node can be merged into a single node during optimization. Currently we can not quantize BatchNormalization by itself, but we can quantize the merged Convolution + BatchNormalization node. Pre-processing is doing just that.\n",
    "\n",
    "After this step, we create a calibration dataset to perform static quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quant_pre_process\n",
    "\n",
    "input_model = \"./exports/mnist.onnx\"\n",
    "output_model = \"./exports/mnist-infer.onnx\"\n",
    "\n",
    "quant_pre_process(input_model, output_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx, onnxruntime\n",
    "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantFormat\n",
    "\n",
    "model_path = \"./exports/mnist-infer.onnx\"\n",
    "quantized_model_path = \"./exports/mnist-quantized.onnx\"\n",
    "\n",
    "class MNISTDataReader(CalibrationDataReader):\n",
    "    def __init__(self, data_folder, num_samples=100):\n",
    "        self.num_samples = num_samples\n",
    "        self.data_folder = data_folder\n",
    "        self.preprocess_flag = True\n",
    "        self.enum_data_dicts = []\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        dataset = torchvision.datasets.MNIST(root=self.data_folder, train=True, transform=self.transform, download=True)\n",
    "        data_loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "        batch_data = []\n",
    "        for i, (img, _) in enumerate(data_loader):\n",
    "            if i >= self.num_samples:\n",
    "                break\n",
    "            batch_data.append(img.numpy())\n",
    "\n",
    "        self.enum_data_dicts = iter([{\"input\": img} for img in batch_data])\n",
    "\n",
    "    def get_next(self):\n",
    "        return next(self.enum_data_dicts, None)\n",
    "\n",
    "calibration_data_folder = \"./data\"\n",
    "calibration_dataset = MNISTDataReader(calibration_data_folder, num_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ONNX model size (MB): 4.60\n",
      "Quantized ONNX model size (MB): 1.16\n",
      "Quantized model saved to ./exports/mnist-quantized.onnx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "quantize_static(\n",
    "    model_path,\n",
    "    quantized_model_path,\n",
    "    calibration_dataset,\n",
    "    quant_format=QuantFormat.QDQ\n",
    ")\n",
    "\n",
    "print(f\"Original ONNX model size (MB): {os.path.getsize(model_path) / (1024 * 1024):.2f}\")\n",
    "print(f\"Quantized ONNX model size (MB): {os.path.getsize(quantized_model_path) / (1024 * 1024):.2f}\")\n",
    "print(f\"Quantized model saved to {quantized_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph main_graph (\n",
      "  %input[FLOAT, 1x1x28x28]\n",
      ") initializers (\n",
      "  %val_3[INT64, 2]\n",
      "  %input_zero_point[INT8, scalar]\n",
      "  %input_scale[FLOAT, scalar]\n",
      "  %conv1.0.weight_zero_point[INT8, scalar]\n",
      "  %conv1.0.weight_scale[FLOAT, scalar]\n",
      "  %conv1.0.weight_quantized[INT8, 32x1x5x5]\n",
      "  %relu_zero_point[INT8, scalar]\n",
      "  %relu_scale[FLOAT, scalar]\n",
      "  %conv2.0.weight_zero_point[INT8, scalar]\n",
      "  %conv2.0.weight_scale[FLOAT, scalar]\n",
      "  %conv2.0.weight_quantized[INT8, 64x32x3x3]\n",
      "  %relu_1_zero_point[INT8, scalar]\n",
      "  %relu_1_scale[FLOAT, scalar]\n",
      "  %fc1.0.weight_zero_point[INT8, scalar]\n",
      "  %fc1.0.weight_scale[FLOAT, scalar]\n",
      "  %fc1.0.weight_quantized[INT8, 512x2304]\n",
      "  %relu_2_zero_point[INT8, scalar]\n",
      "  %relu_2_scale[FLOAT, scalar]\n",
      "  %output_zero_point[INT8, scalar]\n",
      "  %output_scale[FLOAT, scalar]\n",
      "  %fc2.weight_zero_point[INT8, scalar]\n",
      "  %fc2.weight_scale[FLOAT, scalar]\n",
      "  %fc2.weight_quantized[INT8, 10x512]\n",
      "  %conv1.0.bias_quantized[INT32, 32]\n",
      "  %conv1.0.bias_quantized_scale[FLOAT, 1]\n",
      "  %conv1.0.bias_quantized_zero_point[INT32, scalar]\n",
      "  %conv2.0.bias_quantized[INT32, 64]\n",
      "  %conv2.0.bias_quantized_scale[FLOAT, 1]\n",
      "  %conv2.0.bias_quantized_zero_point[INT32, scalar]\n",
      "  %fc1.0.bias_quantized[INT32, 512]\n",
      "  %fc1.0.bias_quantized_scale[FLOAT, 1]\n",
      "  %fc1.0.bias_quantized_zero_point[INT32, scalar]\n",
      "  %fc2.bias_quantized[INT32, 10]\n",
      "  %fc2.bias_quantized_scale[FLOAT, 1]\n",
      "  %fc2.bias_quantized_zero_point[INT32, scalar]\n",
      ") {\n",
      "  %conv1.0.bias = DequantizeLinear(%conv1.0.bias_quantized, %conv1.0.bias_quantized_scale, %conv1.0.bias_quantized_zero_point)\n",
      "  %conv1.0.weight_DequantizeLinear_Output = DequantizeLinear(%conv1.0.weight_quantized, %conv1.0.weight_scale, %conv1.0.weight_zero_point)\n",
      "  %conv2.0.bias = DequantizeLinear(%conv2.0.bias_quantized, %conv2.0.bias_quantized_scale, %conv2.0.bias_quantized_zero_point)\n",
      "  %conv2.0.weight_DequantizeLinear_Output = DequantizeLinear(%conv2.0.weight_quantized, %conv2.0.weight_scale, %conv2.0.weight_zero_point)\n",
      "  %fc1.0.bias = DequantizeLinear(%fc1.0.bias_quantized, %fc1.0.bias_quantized_scale, %fc1.0.bias_quantized_zero_point)\n",
      "  %fc1.0.weight_DequantizeLinear_Output = DequantizeLinear(%fc1.0.weight_quantized, %fc1.0.weight_scale, %fc1.0.weight_zero_point)\n",
      "  %fc2.bias = DequantizeLinear(%fc2.bias_quantized, %fc2.bias_quantized_scale, %fc2.bias_quantized_zero_point)\n",
      "  %fc2.weight_DequantizeLinear_Output = DequantizeLinear(%fc2.weight_quantized, %fc2.weight_scale, %fc2.weight_zero_point)\n",
      "  %input_QuantizeLinear_Output = QuantizeLinear(%input, %input_scale, %input_zero_point)\n",
      "  %input_DequantizeLinear_Output = DequantizeLinear(%input_QuantizeLinear_Output, %input_scale, %input_zero_point)\n",
      "  %relu = Conv[auto_pad = 'NOTSET', dilations = [1, 1], group = 1, pads = [2, 2, 2, 2], strides = [1, 1]](%input_DequantizeLinear_Output, %conv1.0.weight_DequantizeLinear_Output, %conv1.0.bias)\n",
      "  %relu_QuantizeLinear_Output = QuantizeLinear(%relu, %relu_scale, %relu_zero_point)\n",
      "  %relu_DequantizeLinear_Output = DequantizeLinear(%relu_QuantizeLinear_Output, %relu_scale, %relu_zero_point)\n",
      "  %max_pool2d, %val_0 = MaxPool[auto_pad = 'NOTSET', ceil_mode = 0, dilations = [1, 1], kernel_shape = [2, 2], pads = [0, 0, 0, 0], storage_order = 0, strides = [2, 2]](%relu_DequantizeLinear_Output)\n",
      "  %max_pool2d_QuantizeLinear_Output = QuantizeLinear(%max_pool2d, %relu_scale, %relu_zero_point)\n",
      "  %max_pool2d_DequantizeLinear_Output = DequantizeLinear(%max_pool2d_QuantizeLinear_Output, %relu_scale, %relu_zero_point)\n",
      "  %relu_1 = Conv[auto_pad = 'NOTSET', dilations = [1, 1], group = 1, pads = [0, 0, 0, 0], strides = [2, 2]](%max_pool2d_DequantizeLinear_Output, %conv2.0.weight_DequantizeLinear_Output, %conv2.0.bias)\n",
      "  %relu_1_QuantizeLinear_Output = QuantizeLinear(%relu_1, %relu_1_scale, %relu_1_zero_point)\n",
      "  %relu_1_DequantizeLinear_Output = DequantizeLinear(%relu_1_QuantizeLinear_Output, %relu_1_scale, %relu_1_zero_point)\n",
      "  %view = Reshape[allowzero = 0](%relu_1_DequantizeLinear_Output, %val_3)\n",
      "  %view_QuantizeLinear_Output = QuantizeLinear(%view, %relu_1_scale, %relu_1_zero_point)\n",
      "  %view_DequantizeLinear_Output = DequantizeLinear(%view_QuantizeLinear_Output, %relu_1_scale, %relu_1_zero_point)\n",
      "  %relu_2 = Gemm[alpha = 1, beta = 1, transA = 0, transB = 1](%view_DequantizeLinear_Output, %fc1.0.weight_DequantizeLinear_Output, %fc1.0.bias)\n",
      "  %relu_2_QuantizeLinear_Output = QuantizeLinear(%relu_2, %relu_2_scale, %relu_2_zero_point)\n",
      "  %relu_2_DequantizeLinear_Output = DequantizeLinear(%relu_2_QuantizeLinear_Output, %relu_2_scale, %relu_2_zero_point)\n",
      "  %output_QuantizeLinear_Input = Gemm[alpha = 1, beta = 1, transA = 0, transB = 1](%relu_2_DequantizeLinear_Output, %fc2.weight_DequantizeLinear_Output, %fc2.bias)\n",
      "  %output_QuantizeLinear_Output = QuantizeLinear(%output_QuantizeLinear_Input, %output_scale, %output_zero_point)\n",
      "  %output = DequantizeLinear(%output_QuantizeLinear_Output, %output_scale, %output_zero_point)\n",
      "  return %output\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "quantized_model = onnx.load(quantized_model_path)\n",
    "print(onnx.helper.printable_graph(quantized_model.graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Run quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "model_path = \"./exports/mnist.onnx\"\n",
    "quantized_model_path = \"./exports/mnist-quantized.onnx\"\n",
    "\n",
    "session = onnxruntime.InferenceSession(quantized_model_path, providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "mnist_test = torchvision.datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "image, label = mnist_test[0]\n",
    "image_np = image.numpy().astype(np.float32)\n",
    "\n",
    "input_tensor = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "output = session.run([output_name], {input_name: input_tensor})[0]\n",
    "predicted_label = np.argmax(output, axis=1)[0]\n",
    "\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.title(f\"Predicted: {predicted_label} | Actual: {label}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Compare inference speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 Non-Quantized Model Avg Inference Time: 0.18 ms\n",
      "🔥 Quantized Model Avg Inference Time: 0.19 ms\n",
      "🚀 Speedup: 0.94x faster with quantization\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "model_path = \"./exports/mnist.onnx\"\n",
    "quantized_model_path = \"./exports/mnist-quantized.onnx\"\n",
    "\n",
    "execution_provider = \"MPSExecutionProvider\"\n",
    "non_quantized_session = ort.InferenceSession(model_path,  providers=[\"CPUExecutionProvider\"])\n",
    "quantized_session = ort.InferenceSession(quantized_model_path,  providers=[\"CPUExecutionProvider\"])\n",
    "input_name = quantized_session.get_inputs()[0].name\n",
    "output_name = quantized_session.get_outputs()[0].name\n",
    "\n",
    "def benchmark_model(session, input_tensor, runs=50):\n",
    "    times = []\n",
    "    for _ in range(runs):\n",
    "        start_time = time.time()\n",
    "        input_name = session.get_inputs()[0].name\n",
    "        output_name = session.get_outputs()[0].name\n",
    "        session.run([output_name], {input_name: input_tensor})[0]\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "    return np.mean(times) * 1000\n",
    "\n",
    "image, label = mnist_test[0]\n",
    "image_np = image.numpy().astype(np.float32)\n",
    "\n",
    "input_tensor = np.expand_dims(image_np, axis=0)\n",
    "label_tensor = torch.Tensor([label])\n",
    "label_tensor = np.expand_dims(label_tensor, axis=0)\n",
    "\n",
    "non_quantized_time = benchmark_model(non_quantized_session, input_tensor)\n",
    "quantized_time = benchmark_model(quantized_session, input_tensor)\n",
    "\n",
    "# Display results\n",
    "print(f\"🟢 Non-Quantized Model Avg Inference Time: {non_quantized_time:.2f} ms\")\n",
    "print(f\"🔥 Quantized Model Avg Inference Time: {quantized_time:.2f} ms\")\n",
    "print(f\"🚀 Speedup: {non_quantized_time / quantized_time:.2f}x faster with quantization\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 Non-Quantized Model Avg Inference Time: 0.11 ms | Avg Accuracy: 100.00%\n",
      "🔥 Quantized Model Avg Inference Time: 0.11 ms | Avg Accuracy: 87.50%\n",
      "🚀 Speedup: 1.00x faster with quantization\n"
     ]
    }
   ],
   "source": [
    "from src.utils.quantization import *\n",
    "\n",
    "data_sample = next(iter(testloader))\n",
    "model_path = \"./exports/mnist.onnx\"\n",
    "quantized_model_path = \"./exports/mnist-quantized.onnx\"\n",
    "\n",
    "quantized_session_performance_benchmark(model_path, quantized_model_path, data_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
